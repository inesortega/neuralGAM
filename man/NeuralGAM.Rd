% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NeuralGAM.R
\name{neuralGAM}
\alias{neuralGAM}
\title{Fit a neuralGAM model}
\usage{
neuralGAM(
  formula,
  data,
  family = "gaussian",
  num_units = 64,
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mse",
  uncertainty_method = c("none", "aleatoric", "epistemic", "both"),
  alpha = 0.05,
  forward_passes = 100,
  dropout_rate = 0.1,
  validation_split = NULL,
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  verbose = 1,
  ...
)
}
\arguments{
\item{formula}{Model formula. Smooth terms must be wrapped in \code{s(...)}.
You can specify per-term NN settings, e.g.:
\code{y ~ s(x1, num_units = 1024) + s(x3, num_units = c(1024, 512))}.}

\item{data}{Data frame containing the variables.}

\item{family}{Response distribution: \code{"gaussian"}, \code{"binomial"}, \code{"poisson"}.}

\item{num_units}{Default hidden layer sizes for smooth terms (integer or vector).
\strong{Mandatory} unless every \code{s(...)} specifies its own \code{num_units}.}

\item{learning_rate}{Learning rate for Adam optimizer.}

\item{activation}{Activation function for hidden layers. Either a string understood by
\code{tf$keras$activations$get()} or a function.}

\item{kernel_initializer, bias_initializer}{Initializers for weights and biases.}

\item{kernel_regularizer, bias_regularizer, activity_regularizer}{Optional Keras regularizers.}

\item{loss}{Loss function to use.
\itemize{
\item When \code{uncertainty_method} is \code{"aleatoric"} or \code{"both"}: this is the \strong{mean-head loss}
inside \code{make_quantile_loss()}, and can be any Keras built-in (e.g., \code{"mse"}, \code{"mae"},
\code{"huber"}, \code{"logcosh"}) or a custom function.
\item Otherwise: passed directly to \code{keras::compile()}.
}}

\item{uncertainty_method}{Character string indicating the type of predictive uncertainty to estimate.
One of:
\itemize{
\item \code{"none"} (default): no uncertainty estimation.
\item \code{"aleatoric"}: per-term quantile regression (pinball loss) heads (diagnosis only).
\item \code{"epistemic"}: MC Dropout for mean uncertainty (CIs)
\item \code{"both"}: enable both mechanisms.
}}

\item{alpha}{Significance level for prediction intervals, e.g. \code{0.05} for 95\% coverage.}

\item{forward_passes}{Integer. Number of MC-dropout forward passes used when
\code{uncertainty_method \%in\% c("epistemic","both")}.}

\item{dropout_rate}{Dropout probability in smooth-term NNs (0,1).
\itemize{
\item During training: acts as a regularizer.
\item During prediction (if \code{uncertainty_method} includes "epistemic"): enables MC Dropout sampling.
}}

\item{validation_split}{Optional fraction of training data used for validation.}

\item{w_train}{Optional training weights.}

\item{bf_threshold, ls_threshold}{Convergence thresholds for backfitting and local scoring.}

\item{max_iter_backfitting, max_iter_ls}{Maximum iterations for backfitting and local scoring.}

\item{seed}{Random seed.}

\item{verbose}{Verbosity: \code{0} silent, \code{1} progress messages.}

\item{...}{Additional arguments passed to \code{keras::optimizer_adam()}.}
}
\value{
An object of class \code{"neuralGAM"}, a list with elements including:
\describe{
\item{muhat}{ Numeric vector of fitted mean predictions (training data).}
\item{partial}{ Data frame of partial contributions \eqn{g_j(x_j)} per smooth term.}
\item{y}{ Observed response values.}
\item{eta}{ Linear predictor \eqn{\eta = \eta_0 + \sum_j g_j(x_j)}.}
\item{lwr,upr}{ Lower/upper prediction interval bounds (response scale), if
\code{uncertainty_method \%in\% c("aleatoric","both")}. Otherwise \code{NULL}.}
\item{x}{ Training covariates (inputs).}
\item{model}{ List of fitted Keras models, one per smooth term (+ \code{"linear"} if present).}
\item{eta0}{ Intercept estimate \eqn{\eta_0}.}
\item{family}{ Model family.}
\item{stats}{ Data frame of training/validation losses per backfitting iteration.}
\item{mse}{ Training mean squared error.}
\item{formula}{ Parsed model formula (via \code{get_formula_elements()}).}
\item{history}{ List of Keras training histories per term.}
\item{globals}{ Global hyperparameter defaults.}
\item{alpha}{ PI significance level (if trained with uncertainty).}
\item{build_pi}{ Logical; whether the model was trained with quantile heads.}
\item{uncertainty_method}{ Type of predictive uncertainty used ("none","aleatoric","epistemic","both").}
\item{var_epistemic}{ Matrix of per-term epistemic variances (if computed).}
\item{var_aleatoric}{ Matrix of per-term aleatoric variances (if computed).}
}
}
\description{
Fits a Generalized Additive Model where smooth terms are modeled by \code{keras} neural networks.
In addition to point predictions, the model can optionally estimate \strong{uncertainty bands}:
\itemize{
\item \strong{Epistemic uncertainty (confidence bands):} via Monte Carlo Dropout across forward passes.
\item \strong{Residual quantile bands (diagnostic only):} per-term quantile heads trained on partial residuals \eqn{R_j} via quantile regression,
available through \code{predict(type="terms", terms_band="residual")}.
}

These different layers are exposed at prediction/plotting time:
\itemize{
\item \code{interval="mean_ci"}: confidence bands (epistemic) for the fitted mean \eqn{E[Y|X]}.
\item \code{interval="none"}: point predictions only.
\item \code{terms_band="epistemic"}: SE-based bands for smooth functions \eqn{g_j(x_j)}.
\item \code{terms_band="residual"}: per-term diagnostic bands (not CI/PI).
\item \code{terms_band="both"}: both epistemic SE bands and residual diagnostic bands.
}
}
\examples{
\dontrun{
n <- 24500

seed <- 42
set.seed(seed)

x1 <- runif(n, -2.5, 2.5)
x2 <- runif(n, -2.5, 2.5)
x3 <- runif(n, -2.5, 2.5)

f1 <- x1 ** 2
f2 <- 2 * x2
f3 <- sin(x3)
f1 <- f1 - mean(f1)
f2 <- f2 - mean(f2)
f3 <- f3 - mean(f3)

eta0 <- 2 + f1 + f2 + f3
epsilon <- rnorm(n, 0.25)
y <- eta0 + epsilon
train <- data.frame(x1, x2, x3, y)

library(neuralGAM)
# Global architecture
ngam <- neuralGAM(
  y ~ s(x1) + x2,
  data = train,
  num_units = 128
)
ngam
# Per-term architecture
ngam <- neuralGAM(
  y ~ s(x1, num_units = c(128,64), activation = "tanh") +
       s(x2, num_units = 256),
  data = train
)
ngam
# Construct prediction intervals
ngam <- neuralGAM(
  y ~ s(x1) + x2,
  num_units = 128,
  data = train,
  uncertainty_method = "aleatoric",
  alpha = 0.05
)
# Visualize point prediction and prediction intervals using autoplot:
autoplot(ngam, which = "terms", term = "x1", intervals = "prediction")
}
}
\author{
Ines Ortega-Fernandez, Marta Sestelo.
}
