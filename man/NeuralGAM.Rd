% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NeuralGAM.R
\name{neuralGAM}
\alias{neuralGAM}
\title{Fit a \code{neuralGAM} model}
\usage{
neuralGAM(
  formula,
  data,
  num_units,
  family = "gaussian",
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mse",
  validation_split = NULL,
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  verbose = 1,
  ...
)
}
\arguments{
\item{formula}{An object of class "formula": a description of the model to be fitted.
Smooth terms can be added using `s()`.}

\item{data}{A data frame containing the response variable and covariates required by the formula.
Additional terms not present in the formula will be ignored.}

\item{num_units}{Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer with that number of units is used.
If a vector of values is provided, each element defines the number of units in a hidden layer.}

\item{family}{Distribution family to use for fitting. Options are `"gaussian"`, `"binomial"`, `"poisson"`.}

\item{learning_rate}{Learning rate for the neural network optimizer.}

\item{activation}{Activation function for the hidden layers. Defaults to `"relu"`.}

\item{kernel_initializer}{Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (`"glorot_normal"`).}

\item{kernel_regularizer}{Optional regularizer function applied to the kernel weights.}

\item{bias_regularizer}{Optional regularizer function applied to the bias.}

\item{bias_initializer}{Optional initializer for the bias vector.}

\item{activity_regularizer}{Optional regularizer function applied to the output of the layer.}

\item{loss}{Loss function to use during training. Defaults to `"mse"`.}

\item{validation_split}{Numeric between 0 and 1. Fraction of training data to use for validation.}

\item{w_train}{Optional numeric vector of sample weights. If `NULL`, all weights are set to 1.}

\item{bf_threshold}{Numeric convergence threshold for the backfitting algorithm. Default is `0.001`.}

\item{ls_threshold}{Numeric convergence threshold for the local scoring algorithm. Default is `0.1`.}

\item{max_iter_backfitting}{Integer, maximum backfitting iterations. Default is 10.}

\item{max_iter_ls}{Integer, maximum local scoring iterations. Default is 10.}

\item{seed}{Optional integer seed for reproducibility.}

\item{verbose}{Verbosity mode (0 = silent, 1 = print messages). Default is 1.}

\item{...}{Additional parameters passed to `keras::optimizer_adam`.}
}
\value{
A trained `neuralGAM` object with:
\describe{
  \item{muhat}{Predicted response values.}
  \item{partial}{Data frame of partial effects.}
  \item{y}{Observed response values.}
  \item{eta}{Linear predictor values.}
  \item{x}{Model covariates.}
  \item{model}{List of trained neural networks for each term.}
  \item{history}{Training history for each term's model.}
  \item{stats}{Training statistics including loss values.}
  \item{mse}{Mean squared error.}
}
}
\description{
Fits a \code{neuralGAM} model by building a neural network to attend to each covariate.
}
\details{
The function builds one neural network to attend to each feature in `x`,
using the backfitting and local scoring algorithms to fit a weighted additive model
with neural networks as function approximators.
The adjustment of the dependent variable and the weights is determined by the distribution of
the response `y`, adjusted by the `family` parameter.
}
\examples{
\dontrun{
set.seed(42)
n <- 1000
x1 <- runif(n, -2.5, 2.5)
x2 <- runif(n, -2.5, 2.5)
y <- 2 + x1^2 + sin(x2) + rnorm(n, 0.25)
train <- data.frame(x1, x2, y)

ngam <- neuralGAM(y ~ s(x1) + s(x2), data = train,
                 num_units = 64, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 5, max_iter_ls = 5,
                 seed = 42)
summary(ngam)
}

}
\references{
Hastie, T., & Tibshirani, R. (1990). Generalized Additive Models. London: Chapman and Hall.
}
\author{
Ines Ortega-Fernandez, Marta Sestelo
}
