% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NeuralGAM.R
\name{NeuralGAM}
\alias{NeuralGAM}
\title{Fit a NeuralGAM model}
\usage{
NeuralGAM(
  formula,
  data,
  num_units,
  family = "gaussian",
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  ...
)
}
\arguments{
\item{formula}{A GAM formula. You can add smooth terms using \code{s()}.}

\item{data}{A data frame containing the model response variable and covariates
required by the formula. Additional terms not present in the formula will be ignored.}

\item{num_units}{Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer neural network with that number of units is used.
If a list of values is provided, a multi-layer neural network with each element of the list defining
the number of hidden units on each hidden layer is used.}

\item{family}{A description of the link function used in the model
(defaults to \code{gaussian}). Set \code{family="gaussian"} for linear
regression and \code{family="binomial"} for logistic regression.}

\item{learning_rate}{Learning rate for the neural network optimizer.}

\item{activation}{Activation function of the neural network. Defaults to \code{relu}}

\item{kernel_initializer}{Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (\code{glorot_normal}).}

\item{kernel_regularizer}{Optional regularizer function applied to the kernel weights matrix.}

\item{bias_regularizer}{Optional regularizer funciton applied to the bias vector.}

\item{bias_initializer}{Optional initializer for the bias vector.}

\item{activity_regularizer}{Optional regularizer function applied to the output of the layer}

\item{w_train}{Optional sample weights}

\item{bf_threshold}{Convergence criterion of the backfitting algorithm.
Defaults to \code{0.001}}

\item{ls_threshold}{Convergence criterion of the local scoring algorithm.
Defaults to \code{0.1}}

\item{max_iter_backfitting}{An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to \code{10}.}

\item{max_iter_ls}{An integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to \code{10}.}

\item{seed}{A positive integer which specifies the random number generator
seed for algorithms dependent on randomization.}

\item{\ldots}{Additional parameters for the Adam optimizer (see ?keras::optimizer_adam)}
}
\value{
A trained NeuralGAM object. Use \code{summary(ngam)} to see details.
}
\description{
Main function to fit a NeuralGAM model. The function builds one
neural network to attend to each feature in x, using the
backfitting and local scoring algorithms to fit a weighted additive model
using neural networks as function approximators. The adjustment of the
dependent variable and the weights is determined by the distribution of the
response \code{y}, adjusted by the \code{family} parameter.
}
\examples{

n <- 24500
x1 <- runif(n, -2.5, 2.5)
x2 <- runif(n, -2.5, 2.5)
x3 <- runif(n, -2.5, 2.5)

f1 <-x1**2
f2 <- 2*x2
f3 <- sin(x3)
f1 <- f1 - mean(f1)
f2 <- f2 - mean(f2)
f3 <- f3 - mean(f3)

eta0 <- 2 + f1 + f2 + f3
epsilon <- rnorm(n, 0.25)
y <- eta0 + epsilon
train <- data.frame(x1, x2, x3, y, f1, f2, f3)

library(NeuralGAM)
ngam <- NeuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10
                 )

ngam
}
\author{
Ines Ortega-Fernandez, Marta Sestelo.
}
