% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NeuralGAM.R
\name{neuralGAM}
\alias{neuralGAM}
\title{Fit a neuralGAM model}
\usage{
neuralGAM(
  formula,
  data,
  family = "gaussian",
  num_units = 64,
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mse",
  pi_method = c("none", "aleatoric", "epistemic", "both"),
  alpha = 0.05,
  forward_passes = 30,
  dropout_rate = 0.1,
  inner_samples = 20,
  validation_split = NULL,
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  verbose = 1,
  ...
)
}
\arguments{
\item{formula}{Model formula. Smooth terms must be wrapped in \code{s(...)}.
You can specify per-term network settings, e.g.:
\code{y ~ s(x1, num_units = 1024) + s(x3, num_units = c(1024, 512))}.}

\item{data}{Data frame containing the variables.}

\item{family}{Response distribution: \code{"gaussian"}, \code{"binomial"}, \code{"poisson"}.}

\item{num_units}{Default hidden layer sizes for smooth terms (integer or vector).
\strong{Mandatory} unless every \code{s(...)} specifies its own \code{num_units}.}

\item{learning_rate}{Learning rate for Adam optimizer.}

\item{activation}{Activation function for hidden layers.}

\item{kernel_initializer, bias_initializer}{Initializers for weights and biases.}

\item{kernel_regularizer, bias_regularizer, activity_regularizer}{Optional Keras regularizers.}

\item{loss}{Loss function.
\itemize{
\item If \code{pi_method = "none"}: used directly for training.
\item Else: must be \code{"mse"}, \code{"mae"}, or a custom Keras loss function (applies to mean prediction inside PI loss).
}}

\item{pi_method}{Character string indicating the type of uncertainty to estimate in prediction intervals.
Must be one of \code{"none"}, \code{"aleatoric"}, \code{"epistemic"}, or \code{"both"}:
\itemize{
\item \code{"none"}: None (default).
\item \code{"aleatoric"}: Use quantile regression loss to capture data-dependent (heteroscedastic) noise.
\item \code{"epistemic"}: Use MC Dropout with multiple forward passes to capture model uncertainty.
\item \code{"both"}: Combine both quantile estimation and MC Dropout to estimate total predictive uncertainty.
}}

\item{alpha}{PI significance level, e.g. \code{0.05} for 95\% PI.}

\item{dropout_rate}{Numeric in (0,1). Dropout probability applied to hidden layers of each
smooth-term network (when \code{pi_method} is \code{"epistemic"} or \code{"both"}). It serves two purposes:
\itemize{
\item During training: acts as a regularizer to prevent overfitting.
\item During prediction (when \code{pi_method = "epistemic"} or \code{"both"}):
enables Monte Carlo Dropout sampling to approximate epistemic uncertainty.
}
Typical values are between \code{0.1} and \code{0.3}.}

\item{inner_samples}{Integer, number of draws per MC-dropout pass used when combining
aleatoric and epistemic uncertainty (\code{pi_method = "both"}).
For each dropout mask, \code{inner_samples} values are generated from the Normal
approximation defined by the predicted quantile bounds.
Larger values improve stability of the sampled prediction intervals at the cost of speed.}

\item{validation_split}{Optional fraction of training data used for validation.}

\item{w_train}{Optional training weights.}

\item{bf_threshold, ls_threshold}{Convergence thresholds for backfitting and local scoring.}

\item{max_iter_backfitting, max_iter_ls}{Maximum iterations for backfitting and local scoring.}

\item{seed}{Random seed.}

\item{verbose}{Verbosity: \code{0} silent, \code{1} progress messages.}

\item{...}{Additional arguments passed to \code{keras::optimizer_adam()}.}

\item{forward_passess}{Integer, number of forward passess to run MC-dropout when computing
epistemic uncertainty (\code{pi_method = "epistemic"}) or both aleatoric and epistemic.}
}
\value{
An object of class \code{"neuralGAM"}, which is a list containing:
\describe{
\item{muhat}{ Numeric vector of fitted mean predictions on the training data.}
\item{partial}{ List of partial contributions \eqn{g_j(x_j)} for each smooth term.}
\item{y}{ Observed response values.}
\item{eta}{ Numeric vector of the linear predictor \eqn{\eta = \eta_0 + \sum_j g_j(x_j)}.}
\item{lwr}{ Numeric vector of lower prediction interval bounds if neuralGAM was trained with epistemic/aleatoric unecertainty, otherwise \code{NULL}.}
\item{upr}{ Numeric vector of upper prediction interval bounds if neuralGAM was trained with epistemic/aleatoric unecertainty, otherwise \code{NULL}.}
\item{x}{ List of model inputs (covariates) used in training.}
\item{model}{L ist of fitted Keras models, one per smooth term (plus \code{"linear"} if a linear component is present).}
\item{eta0}{ Intercept estimate \eqn{\eta_0}.}
\item{family}{ Model family (\code{"gaussian"}, \code{"binomial"}, \code{"poisson"}).}
\item{stats}{ Data frame of training/validation losses per backfitting iteration.}
\item{mse}{ Training mean squared error.}
\item{formula}{ The original model formula, as parsed by \code{get_formula_elements()}.}
\item{history}{ List of Keras training histories for each fitted term.}
\item{globals}{ List of global default hyperparameters used for architecture and training.}
\item{alpha}{ PI significance level (only relevant when model was trained with uncertainty).}
\item{build_pi}{ Logical; whether the model was trained to produce prediction/confidence intervals.}
\item{pi_method}{ Character string: type of predictive uncertainty estimated}
}
}
\description{
Fits a Generalized Additive Model where the smooth terms are modeled using \code{keras} neural networks.
The model can optionally output \strong{prediction intervals} (lower bound, upper bound, and mean prediction)
using a custom quantile loss (\code{make_quantile_loss()}), or a standard single-output point prediction
using any user-specified loss function.
'
}
\details{
\strong{Defining per-term architectures}
You can pass most Keras architecture/training parameters inside each \code{s(...)} call:

\if{html}{\out{<div class="sourceCode">}}\preformatted{y ~ s(x1, num_units = 512, activation = "tanh") +
    s(x2, num_units = c(256,128), kernel_regularizer = regularizer_l2(1e-4))
}\if{html}{\out{</div>}}

Any term without its own setting will use the global defaults.

**Prediction intervals **
The package supports three PI mechanisms via \code{pi_method}:
\itemize{
\item \code{"aleatoric"}: per-term networks output \emph{lower}, \emph{upper}, and \emph{mean}
using a combined quantile loss + mean loss. This captures data noise (heteroscedasticity).
\item \code{"epistemic"}: per-term networks output a single head for the mean; epistemic
uncertainty is obtained by Monte Carlo (MC) Dropout at prediction time. The line is the
deterministic prediction (dropout \emph{off}); the interval comes from empirical
quantiles across many stochastic forward passes (dropout \emph{on}).
\item \code{"both"}: combines aleatoric and epistemic by running MC Dropout with the
3-output (lower/upper/mean) head and sampling from the induced mixture of Gaussians
to form empirical prediction quantiles.
}
\strong{Centering for partial effects}
For identifiability, each smooth term \eqn{g_j(x_j)} is mean-centered after fitting. When plotting
partial effects (e.g., \code{autoplot(ngam, "x1")}), the associated PI bounds are shifted by the \emph{same}
centering constant so that the band and the smooth share the same baseline. (Widths/variances are
unaffected by this shift.) Full-model predictions on the response scale are not centered.

\strong{MC Dropout controls}
\itemize{
\item \code{dropout_rate}: probability of dropping units in hidden layers. Used as a regularizer
during training and \emph{reused} at prediction time to approximate epistemic uncertainty.
Practical values are in \code{[0.1, 0.3]}.
\item \code{forward_passes}: number of stochastic forward passes with dropout \emph{on} when
\code{pi_method = "epistemic"} or \code{"both"}. Larger values yield smoother, more stable
envelopes (e.g., 300â€“1000).
\item \code{inner_samples}: only used for \code{pi_method = "both"}. For each dropout pass, the
lower/upper quantiles define a local Normal approximation from which \code{inner_samples}
draws are taken; final PIs are empirical quantiles of all draws across passes.
}

\strong{Losses}
\itemize{
\item Aleatoric: lower/upper heads use the pinball (quantile) loss at \eqn{\alpha/2} and \eqn{1-\alpha/2};
the mean head uses the user-chosen mean loss (\code{"mse"} or \code{"mae"}).
\item Epistemic: any mean loss for the single-output head; uncertainty comes from MC Dropout only.
\item Both: quantile + mean losses (as in aleatoric) and MC Dropout; PIs are built by mixture sampling.
}

\strong{Coverage control}
\code{alpha} sets the nominal coverage (e.g., \code{alpha = 0.05} for 95\% PIs). If empirical coverage on a
validation split deviates from target, a simple global scaling of the half-width (conformal-style
calibration) can be applied post hoc.

\strong{Point prediction (\code{pi_method == "none"})}
\itemize{
\item Output: single value per term (no intervals).
\item Loss: exactly as given in \code{loss}.
}
}
\examples{
\dontrun{
n <- 24500

seed <- 42
set.seed(seed)

x1 <- runif(n, -2.5, 2.5)
x2 <- runif(n, -2.5, 2.5)
x3 <- runif(n, -2.5, 2.5)

f1 <- x1 ** 2
f2 <- 2 * x2
f3 <- sin(x3)
f1 <- f1 - mean(f1)
f2 <- f2 - mean(f2)
f3 <- f3 - mean(f3)

eta0 <- 2 + f1 + f2 + f3
epsilon <- rnorm(n, 0.25)
y <- eta0 + epsilon
train <- data.frame(x1, x2, x3, y)

library(neuralGAM)
# Global architecture
ngam <- neuralGAM(
  y ~ s(x1) + x2,
  data = train,
  num_units = 128
)
ngam
# Per-term architecture
ngam <- neuralGAM(
  y ~ s(x1, num_units = c(128,64), activation = "tanh") +
       s(x2, num_units = 256),
  data = train
)
ngam
# Construct prediction intervals
ngam <- neuralGAM(
  y ~ s(x1) + x2,
  num_units = 128,
  data = train,
  pi_method = "aleatoric",
  alpha = 0.05
)
# Visualize point prediction and prediction intervals using autoplot:
autoplot(ngam, "x1")
}
}
\references{
Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.
arXiv preprint arXiv:1412.6980.
Koenker, R., & Bassett, G. (1978). Regression quantiles.
\emph{Econometrica}, 46(1), 33-50.
#'
}
\author{
Ines Ortega-Fernandez, Marta Sestelo.
}
\keyword{internal}
