% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/NeuralGAM.R
\name{NeuralGAM}
\alias{NeuralGAM}
\title{Fit a NeuralGAM model}
\usage{
NeuralGAM(
  x,
  y,
  num_units,
  family = "gaussian",
  learning_rate = 0.001,
  kernel_initializer = "glorot_normal",
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  ...
)
}
\arguments{
\item{x}{A data frame containing all the covariates.}

\item{y}{A numeric vector with the response values.}

\item{num_units}{defines the architecture of each neural network. Use a \code{numeric}
value for shallow neural networks, where \code{num_units} defines the number of hidden units, and
a \code{list()} of \code{numeric} values, where each element defines the number of hidden units on each hidden layer.}

\item{family}{A description of the link function used in the model
(defaults to \code{gaussian}). Set \code{family="gaussian"} for linear
regression and \code{family="binomial"} for logistic regression.}

\item{learning_rate}{learning rate for the neural network optimizer.}

\item{kernel_initializer}{kernel initializer for the Dense layers.
Defaults to Xavier Initializer (\code{glorot_normal}).}

\item{w_train}{optional sample weights.}

\item{bf_threshold}{convergence criterion of the backfitting algorithm.
Defaults to \code{0.001}}

\item{ls_threshold}{convergence criterion of the local scoring algorithm.
Defaults to \code{0.1}}

\item{max_iter_backfitting}{an integer with the maximum number of iterations
of the backfitting algorithm. Defaults to \code{10}.}

\item{max_iter_ls}{an integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to \code{10}.}

\item{\ldots}{Other parameters.}
}
\value{
NeuralGAM object. See  \code{summary(ngam)} for details
}
\description{
Main function to fit a NeuralGAM model. The function builds one
neural network to attend to each to each feature in x, using the
backfitting and local scoring algorithms to fit a weighted additive model
using neural networks as function approximators. The adjustment of the
dependent variable and the weights is determined by the distribution of the
response \code{y}, adjusted by the \code{family} parameter.
}
\examples{

library(NeuralGAM)
data(train)
head(train)
X_train <- train[c("X0", "X1", "X2")]
y_train <- train$y

ngam <- NeuralGAM(
  x = X_train, y = y_train, num_units = 1024, family = "gaussian",
  learning_rate = 0.001, bf_threshold = 0.001,
  max_iter_backfitting = 10, max_iter_ls = 10
)

plot(ngam)

data(test)
X_test <- test[c("X0", "X1", "X2")]
# Obtain linear predictor
eta <- predict(object = ngam, x = X_test, type = "link")
# Obtain each component of the linear predictor separately on each column of a data.frame
terms <- predict(object = ngam, x = X_test, type = "terms")
}
\author{
Ines Ortega-Fernandez, Marta Sestelo.
}
