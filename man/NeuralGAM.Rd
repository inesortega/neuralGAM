% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/neuralGAM-package.R, R/neuralGAM.R
\docType{package}
\name{neuralGAM}
\alias{neuralGAM}
\alias{_PACKAGE}
\alias{neuralGAM-package}
\title{\code{neuralGAM}: interpretable neural network framework based Generalized Additive Models}
\usage{
neuralGAM(
  formula,
  data,
  num_units,
  family = "gaussian",
  learning_rate = 0.001,
  activation = "relu",
  kernel_initializer = "glorot_normal",
  kernel_regularizer = NULL,
  bias_regularizer = NULL,
  bias_initializer = "zeros",
  activity_regularizer = NULL,
  loss = "mean_squared_error",
  w_train = NULL,
  bf_threshold = 0.001,
  ls_threshold = 0.1,
  max_iter_backfitting = 10,
  max_iter_ls = 10,
  seed = NULL,
  ...
)
}
\arguments{
\item{formula}{An object of class "formula": a description of the model to be fitted. You can add smooth terms using \code{s()}.}

\item{data}{A data frame containing the model response variable and covariates
required by the formula. Additional terms not present in the formula will be ignored.}

\item{num_units}{Defines the architecture of each neural network.
If a scalar value is provided, a single hidden layer neural network with that number of units is used.
If a vector of values is provided, a multi-layer neural network with each element of the vector defining
the number of hidden units on each hidden layer is used.}

\item{family}{A description of the link function used in the model
(defaults to \code{gaussian}). Set \code{family="gaussian"} for linear
regression and \code{family="binomial"} for logistic regression.}

\item{learning_rate}{Learning rate for the neural network optimizer.}

\item{activation}{Activation function of the neural network. Defaults to \code{relu}}

\item{kernel_initializer}{Kernel initializer for the Dense layers.
Defaults to Xavier Initializer (\code{glorot_normal}).}

\item{kernel_regularizer}{Optional regularizer function applied to the kernel weights matrix.}

\item{bias_regularizer}{Optional regularizer function applied to the bias vector.}

\item{bias_initializer}{Optional initializer for the bias vector.}

\item{activity_regularizer}{Optional regularizer function applied to the output of the layer}

\item{loss}{Loss function to use during neural network training. Defaults to the mean squared error.}

\item{w_train}{Optional sample weights}

\item{bf_threshold}{Convergence criterion of the backfitting algorithm.
Defaults to \code{0.001}}

\item{ls_threshold}{Convergence criterion of the local scoring algorithm.
Defaults to \code{0.1}}

\item{max_iter_backfitting}{An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to \code{10}.}

\item{max_iter_ls}{An integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to \code{10}.}

\item{seed}{A positive integer which specifies the random number generator
seed for algorithms dependent on randomization.}

\item{\ldots}{Additional parameters for the Adam optimizer (see ?keras::optimizer_adam)}
}
\value{
A trained neuralGAM object. Use \code{summary(ngam)} to see details.
}
\description{
This package provides a method for fitting (deep) neural networks based on Generalized Additive Models
to obtain a highly accurate and explainable deep learning model. The methodology
can be applied both in regression and classification frameworks.

Main function to fit a neuralGAM model. The function builds one
neural network to attend to each feature in x, using the
backfitting and local scoring algorithms to fit a weighted additive model
using neural networks as function approximators. The adjustment of the
dependent variable and the weights is determined by the distribution of the
response \code{y}, adjusted by the \code{family} parameter.
}
\details{
\tabular{ll}{ Package: \tab neuralGAM \cr Type: \tab Package\cr
 License: \tab MIT + file LICENSE\cr}

\code{neuralGAM} is designed along lines similar to those of other \code{R}
packages. This software allows the user to fit a set of independent neural networks
to estimate the contribution of each feature to the response variable, obtaining a
highly accurate and explainable deep learning model.
The package provides \code{neuralGAM()} function that fits a set of independent
neural networks with a given architecture based on the number of units per layer.

For a listing of all routines in the \code{neuralGAM} package type:
\code{library(help="neuralGAM")}.
}
\examples{

n <- 24500

seed <- 42
set.seed(seed)

x1 <- runif(n, -2.5, 2.5)
x2 <- runif(n, -2.5, 2.5)
x3 <- runif(n, -2.5, 2.5)

f1 <- x1 ** 2
f2 <- 2 * x2
f3 <- sin(x3)
f1 <- f1 - mean(f1)
f2 <- f2 - mean(f2)
f3 <- f3 - mean(f3)

eta0 <- 2 + f1 + f2 + f3
epsilon <- rnorm(n, 0.25)
y <- eta0 + epsilon
train <- data.frame(x1, x2, x3, y)

library(neuralGAM)
ngam <- neuralGAM(y ~ s(x1) + x2 + s(x3), data = train,
                 num_units = 1024, family = "gaussian",
                 activation = "relu",
                 learning_rate = 0.001, bf_threshold = 0.001,
                 max_iter_backfitting = 10, max_iter_ls = 10,
                 seed = seed
                 )

ngam
}
\references{
### Main Paper NeuralGAM
}
\author{
Ines Ortega-Fernandez and Marta Sestelo

Ines Ortega-Fernandez, Marta Sestelo.
}
\keyword{internal}
