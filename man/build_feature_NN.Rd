% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build_feature_NN.R
\name{build_feature_NN}
\alias{build_feature_NN}
\title{Build and compile a single Neural Network}
\usage{
build_feature_NN(
  num_units,
  learning_rate = 0.001,
  kernel_initializer = "glorot_normal",
  ...
)
}
\arguments{
\item{num_units}{defines the architecture of each neural network:
* `numeric` value: for shallow neural networks, number of hidden units.
* `list()` of `numeric` values: number of hidden units on each layer
(i.e. \code{list(32,32,32)} generates a DNN
with three layers and \code{32} neurons per layer).}

\item{learning_rate}{learning rate for the neural network optimizer.}

\item{kernel_initializer}{kernel initializer for the Dense layers.
Defaults to Xavier Initializer (\code{glorot_normal}).}

\item{...}{
  Arguments passed on to \code{\link[=fit_NeuralGAM]{fit_NeuralGAM}}
  \describe{
    \item{\code{x}}{A data frame containing all the covariates.}
    \item{\code{y}}{A numeric vector with the response values.}
    \item{\code{family}}{A description of the link function used in the model
(defaults to \code{gaussian})
 * `gaussian` for linear regression.
 * `binomial` for logistic regression.}
    \item{\code{w_train}}{optional sample weights.}
    \item{\code{bf_threshold}}{convergence criterion of the backfitting algorithm.
Defaults to \code{0.001}}
    \item{\code{ls_threshold}}{convergence criterion of the local scoring algorithm.
Defaults to \code{0.1}}
    \item{\code{max_iter_backfitting}}{an integer with the maximum number of iterations
of the backfitting algorithm. Defaults to \code{10}.}
    \item{\code{max_iter_ls}}{an integer with the maximum number of iterations of the
local scoring Algorithm. Defaults to \code{10}.}
  }}
}
\value{
compiled Neural Network
}
\description{
Builds and compiles a neural network using the keras library.
The architecture of the neural network is configurable using the
\code{"num_units"} parameter. A single integer with the number of hidden
units builds a shallow (single layer) neural network. Deep Neural Networks
can be built by specifying the size of each hidden layer in the
 \code{"num_units"} parameter. For example, \code{"list(32,32,32)"} generates
 a DNN with three layers and 32 neurons per layer.
}
\examples{

# Build a Shallow NN with 32 hidden units:
model <- build_feature_NN(num_units=32, learning_rate=0.001,
kernel_initializer="glorot_normal")

# Build a Deep NN with three hidden layers with 32 hidden units on each layer
model <- build_feature_NN(num_units=list(32,32,32),
learning_rate=0.001, kernel_initializer="glorot_normal")

}
\references{
Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
}
\author{
Ines Ortega-Fernandez, Marta Sestelo and Nora M. Villanueva.
}
