% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/build_feature_NN.R
\name{build_feature_NN}
\alias{build_feature_NN}
\title{Build and compile a single Neural Network}
\usage{
build_feature_NN(
  num_units,
  learning_rate = 0.001,
  kernel_initializer = "glorot_normal",
  ...
)
}
\arguments{
\item{num_units}{number of hidden units (for shallow neural networks) or
list of hidden units per layer}

\item{learning_rate}{learning rate for the Adam optimizer (Kingma, 2014).
Defaults to \code{0.001}}

\item{kernel_initializer}{kernel initializer for the Dense layers.
Defaults to Xavier Initializer}

\item{\ldots}{Other options.}
}
\value{
compiled Neural Network
}
\description{
Builds and compiles a neural network using the keras library.
The architecture of the neural network is configurable using the
\code{"num_units"} parameter. A single integer with the number of hidden
units builds a shallow (single layer) neural network. Deep Neural Networks
can be built by specifying the size of each hidden layer in the
 \code{"num_units"} parameter. For example, \code{"list(32,32,32)"} generates
 a DNN with three layers and 32 neurons per layer.
}
\examples{

# Build a Shallow NN with 32 hidden units:
model <- build_feature_NN(num_units=32, learning_rate=0.001,
kernel_initializer="glorot_normal")

# Build a Deep NN with three hidden layers with 32 hidden units on each layer
model <- build_feature_NN(num_units=list(32,32,32),
learning_rate=0.001, kernel_initializer="glorot_normal")

}
\references{
Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980.
}
\author{
Ines Ortega-Fernandez, Marta Sestelo and Nora M. Villanueva.
}
