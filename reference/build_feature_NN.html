<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0"><title>Build and compile a neural network feature model — build_feature_NN • neuralGAM</title><!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous"><script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css"><script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous"><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet"><script src="../pkgdown.js"></script><meta property="og:title" content="Build and compile a neural network feature model — build_feature_NN"><meta property="og:description" content="Builds and compiles a keras neural network for a single smooth term in a
neuralGAM model.
The network can optionally be configured to output symmetric prediction intervals
(lower bound, upper bound, and mean prediction) using a custom quantile loss
(make_quantile_loss()), or a standard single-output point prediction using
any user-specified loss function.
When uncertainty_method is aleatoric or both the model outputs three units corresponding to the
lower bound, upper bound, and mean prediction, and is compiled with
make_quantile_loss(alpha, mean_loss, ...). In any other case, the model
outputs a single unit (point prediction) and uses the loss function provided in loss."><!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--></head><body data-spy="scroll" data-target="#toc">


    <div class="container template-reference-topic">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">neuralGAM</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">2.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav"><li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../news/index.html">Changelog</a>
</li>
      </ul><ul class="nav navbar-nav navbar-right"><li>
  <a href="https://github.com/inesortega/neuralGAM/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul></div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header">
    <h1>Build and compile a neural network feature model</h1>
    <small class="dont-index">Source: <a href="https://github.com/inesortega/neuralGAM/blob/v2.0.0/R/build_feature_NN.R" class="external-link"><code>R/build_feature_NN.R</code></a></small>
    <div class="hidden name"><code>build_feature_NN.Rd</code></div>
    </div>

    <div class="ref-description">
    <p>Builds and compiles a <code>keras</code> neural network for a single smooth term in a
<code>neuralGAM</code> model.</p>
<p>The network can optionally be configured to output <strong>symmetric prediction intervals</strong>
(lower bound, upper bound, and mean prediction) using a custom quantile loss
(<code>make_quantile_loss()</code>), or a standard single-output point prediction using
any user-specified loss function.</p>
<p>When <code>uncertainty_method</code> is <code>aleatoric</code> or <code>both</code> the model outputs three units corresponding to the
lower bound, upper bound, and mean prediction, and is compiled with
<code>make_quantile_loss(alpha, mean_loss, ...)</code>. In any other case, the model
outputs a single unit (point prediction) and uses the loss function provided in <code>loss</code>.</p>
    </div>

    <div id="ref-usage">
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">build_feature_NN</span><span class="op">(</span></span>
<span>  <span class="va">num_units</span>,</span>
<span>  learning_rate <span class="op">=</span> <span class="fl">0.001</span>,</span>
<span>  activation <span class="op">=</span> <span class="st">"relu"</span>,</span>
<span>  kernel_initializer <span class="op">=</span> <span class="st">"glorot_normal"</span>,</span>
<span>  kernel_regularizer <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  bias_regularizer <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  bias_initializer <span class="op">=</span> <span class="st">"zeros"</span>,</span>
<span>  activity_regularizer <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  loss <span class="op">=</span> <span class="st">"mse"</span>,</span>
<span>  name <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  alpha <span class="op">=</span> <span class="fl">0.05</span>,</span>
<span>  w_mean <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  order_penalty_lambda <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  uncertainty_method <span class="op">=</span> <span class="st">"none"</span>,</span>
<span>  dropout_rate <span class="op">=</span> <span class="fl">0.1</span>,</span>
<span>  seed <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  <span class="va">...</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div id="arguments">
    <h2>Arguments</h2>


<dl><dt id="arg-num-units">num_units<a class="anchor" aria-label="anchor" href="#arg-num-units"></a></dt>
<dd><p>Integer or vector of integers. Number of units in the hidden
layer(s). If a vector is provided, multiple dense layers are added sequentially.</p></dd>


<dt id="arg-learning-rate">learning_rate<a class="anchor" aria-label="anchor" href="#arg-learning-rate"></a></dt>
<dd><p>Numeric. Learning rate for the Adam optimizer.</p></dd>


<dt id="arg-activation">activation<a class="anchor" aria-label="anchor" href="#arg-activation"></a></dt>
<dd><p>Character string or function. Activation function to use in hidden layers.
If character, it must be valid for <code>tf$keras$activations$get()</code>.</p></dd>


<dt id="arg-kernel-initializer">kernel_initializer<a class="anchor" aria-label="anchor" href="#arg-kernel-initializer"></a></dt>
<dd><p>Keras initializer object or string. Kernel initializer for dense layers.</p></dd>


<dt id="arg-kernel-regularizer">kernel_regularizer<a class="anchor" aria-label="anchor" href="#arg-kernel-regularizer"></a></dt>
<dd><p>Optional Keras regularizer for kernel weights.</p></dd>


<dt id="arg-bias-regularizer">bias_regularizer<a class="anchor" aria-label="anchor" href="#arg-bias-regularizer"></a></dt>
<dd><p>Optional Keras regularizer for bias terms.</p></dd>


<dt id="arg-bias-initializer">bias_initializer<a class="anchor" aria-label="anchor" href="#arg-bias-initializer"></a></dt>
<dd><p>Keras initializer object or string. Initializer for bias terms.</p></dd>


<dt id="arg-activity-regularizer">activity_regularizer<a class="anchor" aria-label="anchor" href="#arg-activity-regularizer"></a></dt>
<dd><p>Optional Keras regularizer for layer activations.</p></dd>


<dt id="arg-loss">loss<a class="anchor" aria-label="anchor" href="#arg-loss"></a></dt>
<dd><p>Loss function to use.</p><ul><li><p>When <code>uncertainty_method</code> is <code>aleatoric</code> or <code>both</code>, this is the <strong>mean-head loss</strong> inside
<code>make_quantile_loss()</code> and can be any <code>keras</code> built-in loss name (e.g., <code>"mse"</code>, <code>"mae"</code>, <code>"huber"</code>,
<code>"logcosh"</code>, ...) or a custom function.</p></li>
<li><p>In any other case, this is used directly in <code>compile()</code>.</p></li>
</ul></dd>


<dt id="arg-name">name<a class="anchor" aria-label="anchor" href="#arg-name"></a></dt>
<dd><p>Optional character string. Name assigned to the model.</p></dd>


<dt id="arg-alpha">alpha<a class="anchor" aria-label="anchor" href="#arg-alpha"></a></dt>
<dd><p>Numeric. Desired significance level for <strong>symmetric</strong> prediction intervals.
Defaults to 0.05 (i.e., 95% PI using quantiles alpha/2 and 1-alpha/2).</p></dd>


<dt id="arg-w-mean">w_mean<a class="anchor" aria-label="anchor" href="#arg-w-mean"></a></dt>
<dd><p>Non-negative numeric. Weight for the mean-head loss within the composite PI loss.</p></dd>


<dt id="arg-order-penalty-lambda">order_penalty_lambda<a class="anchor" aria-label="anchor" href="#arg-order-penalty-lambda"></a></dt>
<dd><p>Non-negative numeric. Strength of a soft monotonicity penalty
<code>ReLU(lwr - upr)</code> to discourage interval inversions.</p></dd>


<dt id="arg-uncertainty-method">uncertainty_method<a class="anchor" aria-label="anchor" href="#arg-uncertainty-method"></a></dt>
<dd><p>Character string indicating the type of uncertainty to estimate in prediction intervals.
Must be one of <code>"none"</code>, <code>"aleatoric"</code>, <code>"epistemic"</code>, or <code>"both"</code>.</p></dd>


<dt id="arg-dropout-rate">dropout_rate<a class="anchor" aria-label="anchor" href="#arg-dropout-rate"></a></dt>
<dd><p>Numeric in (0,1). Dropout rate used when <code>uncertainty_method %in% c("epistemic","both")</code>.</p></dd>


<dt id="arg-seed">seed<a class="anchor" aria-label="anchor" href="#arg-seed"></a></dt>
<dd><p>Random seed.</p></dd>


<dt id="arg--">...<a class="anchor" aria-label="anchor" href="#arg--"></a></dt>
<dd><p>Arguments passed on to <code><a href="NeuralGAM.html">neuralGAM</a></code></p><dl><dt><code>formula</code></dt>
<dd><p>Model formula. Smooth terms must be wrapped in <code>s(...)</code>.
You can specify per-term NN settings, e.g.:
<code>y ~ s(x1, num_units = 1024) + s(x3, num_units = c(1024, 512))</code>.</p></dd>

    <dt><code>data</code></dt>
<dd><p>Data frame containing the variables.</p></dd>

    <dt><code>family</code></dt>
<dd><p>Response distribution: <code>"gaussian"</code>, <code>"binomial"</code>, <code>"poisson"</code>.</p></dd>

    <dt><code>kernel_initializer,bias_initializer</code></dt>
<dd><p>Initializers for weights and biases.</p></dd>

    <dt><code>kernel_regularizer,bias_regularizer,activity_regularizer</code></dt>
<dd><p>Optional Keras regularizers.</p></dd>

    <dt><code>forward_passes</code></dt>
<dd><p>Integer. Number of MC-dropout forward passes used when
<code>uncertainty_method %in% c("epistemic","both")</code>.</p></dd>

    <dt><code>validation_split</code></dt>
<dd><p>Optional fraction of training data used for validation.</p></dd>

    <dt><code>w_train</code></dt>
<dd><p>Optional training weights.</p></dd>

    <dt><code>bf_threshold</code></dt>
<dd><p>Convergence criterion of the backfitting algorithm. Defaults to <code>0.001</code></p></dd>

    <dt><code>ls_threshold</code></dt>
<dd><p>Convergence criterion of the local scoring algorithm. Defaults to <code>0.1</code></p></dd>

    <dt><code>max_iter_backfitting</code></dt>
<dd><p>An integer with the maximum number of iterations
of the backfitting algorithm. Defaults to <code>10</code>.</p></dd>

    <dt><code>max_iter_ls</code></dt>
<dd><p>An integer with the maximum number of iterations of the local scoring Algorithm. Defaults to <code>10</code>.</p></dd>

    <dt><code>verbose</code></dt>
<dd><p>Verbosity: <code>0</code> silent, <code>1</code> progress messages.</p></dd>


</dl></dd>

</dl></div>
    <div id="value">
    <h2>Value</h2>
    <p>A compiled <code>keras_model</code> object ready for training.</p>
    </div>
    <div id="details">
    <h2>Details</h2>
    <p><strong>Prediction interval mode (<code>uncertainty_method %in% c("aleatoric", "both")</code>)</strong>:</p><ul><li><p>Output layer has 3 units:</p><ul><li><p><code>lwr</code>: lower bound, \(\tau = \alpha/2\)</p></li>
<li><p><code>upr</code>: upper bound, \(\tau = 1 - \alpha/2\)</p></li>
<li><p><code>y_hat</code>: mean prediction</p></li>
</ul></li>
<li><p>Loss function is <code>make_quantile_loss()</code> which combines two pinball losses
(for lower and upper quantiles) with the chosen mean prediction loss and an optional
non-crossing penalty.</p></li>
</ul><p><strong>Point prediction mode (<code>uncertainty_method %in% c("none", "epistemic")</code>)</strong>:</p><ul><li><p>Output layer has 1 unit: point prediction only.</p></li>
<li><p>Loss function is the one passed in <code>loss</code>.</p></li>
</ul></div>
    <div id="references">
    <h2>References</h2>
    <p>Kingma, D. P., &amp; Ba, J. (2014). Adam: A method for stochastic optimization.
arXiv:1412.6980.
Koenker, R., &amp; Bassett Jr, G. (1978). Regression quantiles. <em>Econometrica</em>, 46(1), 33-50.</p>
    </div>
    <div id="author">
    <h2>Author</h2>
    <p>Ines Ortega-Fernandez, Marta Sestelo</p>
    </div>

  </div>
  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">
    <nav id="toc" data-toggle="toc" class="sticky-top"><h2 data-toc-skip>Contents</h2>
    </nav></div>
</div>


      <footer><div class="copyright">
  <p></p><p>Developed by Ines Ortega-Fernandez, Marta Sestelo.</p>
</div>

<div class="pkgdown">
  <p></p><p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

      </footer></div>






  </body></html>

